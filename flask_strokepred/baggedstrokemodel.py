# -*- coding: utf-8 -*-
"""baggedstrokemodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Gp-D8SVUOQ-WmcDcfQbvwIKkBngAy3rl
"""

import pandas as pd
import numpy as np
import joblib
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder, PolynomialFeatures
from sklearn.model_selection import StratifiedKFold, train_test_split
from sklearn.metrics import roc_auc_score, roc_curve, auc, RocCurveDisplay, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier
from sklearn.utils import class_weight
from imblearn.over_sampling import SMOTE

"""DATA PREPROCESSING"""

# Read the CSV file
train = pd.read_csv('stroketrain.csv')

# Drop null values
train.dropna(inplace=True)
train.iloc[0:10]

train.info()

"""DATA EXPLORATION"""

round(100*train['stroke'].value_counts() / train.shape[0], 2)

sns.countplot(x = 'stroke', hue = 'stroke', data = train);

# Data exploration
def plot_histogram(ax, data, x_label, hue, title):
    sns.histplot(ax=ax, x=data, hue=hue, fill=True, data=train)
    ax.set_xlabel(x_label, fontsize=12)
    ax.set_ylabel('Count', fontsize=12)
    ax.set_title(title, fontsize=14)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))
plot_histogram(axes[0], 'age', 'Age', 'stroke', 'Age Distribution')
plot_histogram(axes[1], 'avg_glucose_level', 'Average Glucose Level', 'stroke', 'Average Glucose Level Distribution')
plot_histogram(axes[2], 'bmi', 'BMI', 'stroke', 'BMI Distribution')
plt.show()

def plot_categorical(data, x_col, y_col):
    plot = sns.catplot(data=data, x=x_col, y=y_col, kind='point', color='pink')
    plot.set_xlabels(x_col, fontsize=12)
    plot.set_ylabels('Stroke Likelihood', fontsize=12)
    return plot

categorical_columns = ['gender', 'hypertension', 'heart_disease', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
for col in categorical_columns:
    plot = plot_categorical(train, col, 'stroke')
    if len(train[col].unique()) <= 2:  # Combine figures with only one row
        plot.fig.subplots_adjust(top=0.8)  # Adjust the top margin for combined figures
    else:
        plot.set_xticklabels(rotation=30)
plt.show()

# Encode categorical columns
le_dict = {}
for col in train.columns:
    if train[col].dtype == 'object':
        le = LabelEncoder()
        train[col] = le.fit_transform(train[col])
        le_dict[col] = le
train.iloc[0:10]

# Data preparation
X = train.drop(columns=['stroke'])
Y = train['stroke']
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.fit_transform(X_test)

# Resampling using SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, Y_train_resampled = smote.fit_resample(X_train, Y_train)

"""MODELLING"""

# Calculate class weights for cost-sensitive learning
weights = class_weight.compute_sample_weight(class_weight='balanced', y=Y_train_resampled)

# Define base models
rf_base = RandomForestClassifier(max_depth=5, min_samples_leaf=8, min_samples_split=5, n_estimators=300, class_weight='balanced', random_state=42)
lr_base = LogisticRegression(C=0.6, penalty='l1', solver='saga', max_iter=10000, class_weight='balanced', random_state=42)
gb_base = GradientBoostingClassifier(n_estimators=300, learning_rate=0.07, max_depth=5, min_samples_split=5, subsample=0.6, random_state=42)

# Train the Gradient Boosting model with sample weights for cost-sensitive learning
gb_base.fit(X_train_resampled, Y_train_resampled, sample_weight=weights)

# Define bagging classifiers
bagged_RF = BaggingClassifier(base_estimator=rf_base, n_estimators=5, random_state=42)
bagged_LR = BaggingClassifier(base_estimator=lr_base, n_estimators=5, random_state=42)
bagged_GB = BaggingClassifier(base_estimator=gb_base, n_estimators=5, random_state=42)

# Cross-validation and training for each base model
def cross_validate_and_train(bagged_model, X, Y):
    cv_scores, roc_auc_scores = [], []
    skf = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
    for train_ix, test_ix in skf.split(X, Y):
        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]
        Y_train, Y_test = Y.iloc[train_ix], Y.iloc[test_ix]
        bagged_model.fit(X_train, Y_train)
        preds = bagged_model.predict_proba(X_test)[:, 1]
        roc_auc_scores.append(roc_auc_score(Y_test, preds))
    cv_scores.append(np.mean(roc_auc_scores))
    return np.mean(cv_scores)

rf_cv_score = cross_validate_and_train(bagged_RF, X, Y)
lr_cv_score = cross_validate_and_train(bagged_LR, X, Y)
gb_cv_score = cross_validate_and_train(bagged_GB, X, Y)

# Display CV scores
print('Random Forest Bagged ROC-AUC:', rf_cv_score)
print('Logistic Regression Bagged ROC-AUC:', lr_cv_score)
print('Gradient Boosting Bagged ROC-AUC:', gb_cv_score)

# Making predictions on the entire train dataset for each model
bagged_RF.fit(X, Y)
train_preds_RF = pd.DataFrame({'stroke': Y, 'stroke_pred': bagged_RF.predict_proba(X)[:, 1]})
train_preds_RF.head()

bagged_LR.fit(X, Y)
train_preds_LR = pd.DataFrame({'stroke': Y, 'stroke_pred': bagged_LR.predict_proba(X)[:, 1]})
train_preds_LR.head()

bagged_GB.fit(X, Y)
train_preds_GB = pd.DataFrame({'stroke': Y, 'stroke_pred': bagged_GB.predict_proba(X)[:, 1]})
train_preds_GB.head()

# Plot ROC curves for each bagged classifier
def plot_roc_curve(train_preds, title):
    RocCurveDisplay.from_predictions(train_preds['stroke'], train_preds['stroke_pred'])
    plt.title(title)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.show()

plot_roc_curve(train_preds_RF, 'Random Forest Bagging ROC-AUC Curve on Train')
plot_roc_curve(train_preds_LR, 'Logistic Regression Bagging ROC-AUC Curve on Train')
plot_roc_curve(train_preds_GB, 'Gradient Boosting Bagging ROC-AUC Curve on Train')

"""COMBINATION OF ALL 4 MODELS

"""

# Combine predictions from all models for the test set
ensemble_predictions = (bagged_RF.predict_proba(X_test)[:, 1] +
                        bagged_LR.predict_proba(X_test)[:, 1] +
                        bagged_GB.predict_proba(X_test)[:, 1]) / 3

# Create a DataFrame with the ensemble predictions
df_ensemble_predictions = pd.DataFrame({'Ensemble_Predicted': ensemble_predictions})

# Show the actual values and ensemble predictions
df_combined_predictions = pd.concat([pd.DataFrame({'Actual': Y}), df_ensemble_predictions], axis=1)
print('Combined Predictions:\n', df_combined_predictions)

# ROC AUC Calculation
fpr, tpr, _ = roc_curve(Y_test, ensemble_predictions)
roc_auc = auc(fpr, tpr)
print(f'ROC AUC Score: {roc_auc}')

# Plotting the ROC Curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Ensemble models')
plt.legend(loc="lower right")
plt.show()

# Find Optimal Threshold
precisions, recalls, thresholds = precision_recall_curve(Y_test, ensemble_predictions)
optimal_idx = np.argmax(np.sqrt(recalls * precisions))
optimal_threshold = thresholds[optimal_idx]

ensemble_class_labels = np.where(ensemble_predictions > optimal_threshold, 1, 0)

# Evaluate the model
precision = precision_score(Y_test, ensemble_class_labels)
recall = recall_score(Y_test, ensemble_class_labels)
f1 = f1_score(Y_test, ensemble_class_labels)
accuracy = accuracy_score(Y_test, ensemble_class_labels)

# Display results
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1}")
print(f"Accuracy: {accuracy}")

# Confusion Matrix
cm = confusion_matrix(Y_test, ensemble_class_labels)
print("Confusion Matrix:")
print(cm)

# Plotting the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

from sklearn.base import BaseEstimator

class EnsembleModel(BaseEstimator):
    def __init__(self, models):
        self.models = models  # A list of the individual models

    def fit(self, X, y):
        # Fit each of the models on the data
        for model in self.models:
            model.fit(X, y)
        return self

    def predict_proba(self, X):
        # Averaging the predict_proba of each of the models
        predictions = [model.predict_proba(X)[:, 1] for model in self.models]
        return np.mean(predictions, axis=0)

    def predict(self, X):
        # Convert probabilities to final predictions
        probabilities = self.predict_proba(X)
        return np.where(probabilities > 0.5, 1, 0)  # Using 0.5 as threshold for binary classification

# Example of creating and saving the ensemble model
ensemble_model = EnsembleModel(models=[bagged_RF, bagged_LR, bagged_GB])

"""SAVING USING JOBLIB"""

# Save the scaler to a file
with open('bagged_scaler.pkl', 'wb') as f:
    joblib.dump(scaler, f)

# Save the ensemble model using joblib
with open('bagged_ensemble_model.pkl', 'wb') as f:
    joblib.dump(ensemble_model, f)